了解一些基础知识，如 HTTP 原理、网页的基础知识、爬虫的基本原理、Cookies 的基本原理等。

## HTTP 基本原理

URL 的开头会有 http 或 https，这个就是访问资源需要的协议类型，有时还会看到 ftp、sftp、smb（协议类型） 开头的 URL

- HTTP（Hyper Text Transfer Protocol，超文本传输协议），用于从网络传输超文本数据到本地浏览器的传送协议，它能保证传送高效而准确地传送超文本文档。

- HTTPS（Hyper Text Transfer Protocol over Secure Socket Layer），是以安全为目标的 HTTP 通道。

HTTPS 的安全基础是 SSL，因此通过它传输的内容都是经过 SSL 加密的，它的主要作用可以分为两种：
- 建立一个信息安全通道，来保证数据传输的安全。 
- 确认网站的真实性，凡是使用了 https 的网站，都可以通过点击浏览器地址栏的锁头标志来查看网站认证之后的真实信息，也可以通过 CA 机构颁发的安全签章来查询。

如果要爬取这样的站点，就需要设置忽略证书的选项，否则会提示 SSL 链接错误。

### 请求
由客户端向服务端发出，可以分为 4 部分内容：
- 请求方法（Request Method）
- 请求的网址（Request URL）
- 请求头（Request Headers）
- 请求体（Request Body）

**请求方法** 

常见的请求方法有两种：GET 和 POST。 
- 在浏览器中直接输入 URL 并回车，这便发起了一个 GET 请求，请求的参数会直接包含到 URL 里。
- POST 请求大多在表单提交时发起。比如，对于一个登录表单，通常会发起一个 POST 请求，其数据通常以表单的形式传输，而不会体现在 URL 中。 

> GET 请求提交的数据最多只有 1024 字节，而 POST 方式没有限制。 
> 
> 一般来说，登录时，需要提交用户名和密码，其中包含了敏感信息，最好以 POST 方式发送。上传文件时，由于文件内容比较大，也会选用 POST 方式。

其它请求方法（HEAD、PUT、DELETE、OPTIONS、CONNECT、TRACE）等。
- HEAD	类似于 GET 请求，只不过返回的响应中没有具体的内容，用于获取报头 
- PUT	从客户端向服务器传送的数据取代指定文档中的内容 
- DELETE	请求服务器删除指定的页面 
- CONNECT	把服务器当作跳板，让服务器代替客户端访问其他网页 
- OPTIONS	允许客户端查看服务器的性能 
- TRACE	回显服务器收到的请求，主要用于测试或诊断

**请求头**

request headers，用来说明服务器要使用的附加信息，比较重要的信息有 Cookie、Referer、User-Agent 等。
- Accept：请求报头域，用于指定客户端可接受哪些类型的信息。 
- Accept-Language：指定客户端可接受的语言类型。 
- Accept-Encoding：指定客户端可接受的内容编码。 
- Host：用于指定请求资源的主机 IP 和端口号，其内容为请求 URL 的原始服务器或网关的位置。从 HTTP 1.1 版本开始，请求必须包含此内容。
- Cookie：也常用复数形式 Cookies，这是网站为了辨别用户进行会话跟踪而存储在用户本地的数据。它的主要功能是维持当前访问会话。例如，我们输入用户名和密码成功登录某个网站后，服务器会用会话保存登录状态信息，后面我们每次刷新或请求该站点的其他页面时，会发现都是登录状态，这就是 Cookies 的功劳。Cookies 里有信息标识了我们所对应的服务器的会话，每次浏览器在请求该站点的页面时，都会在请求头中加上 Cookies 并将其发送给服务器，服务器通过 Cookies 识别出是我们自己，并且查出当前状态是登录状态，所以返回结果就是登录之后才能看到的网页内容。 
- Referer：此内容用来标识这个请求是从哪个页面发过来的，服务器可以拿到这一信息并做相应的处理，如做来源统计、防盗链处理等。 
- User-Agent：简称 UA，它是一个特殊的字符串头，可以使服务器识别客户使用的操作系统及版本、浏览器及版本等信息。在做爬虫时加上此信息，可以伪装为浏览器；如果不加，很可能会被识别出为爬虫。 
- Content-Type：也叫互联网媒体类型（Internet Media Type）或者 MIME 类型，在 HTTP 协议消息头中，它用来表示具体请求中的媒体类型信息。例如，text/html 代表 HTML 格式，image/gif 代表 GIF 图片，application/json 代表 JSON 类型，更多对应关系可以查看此对照表：http://tool.oschina.net/commons。

> 请求头是请求的重要组成部分，在写爬虫时，大部分情况下都需要设定请求头。
 
**请求体**

请求体（体现在*From Data*）一般承载的内容是 POST 请求中的表单数据，而对于 GET 请求，请求体则为空。

例如，这里我登录 GitHub 时捕获到的请求和响应如图 2-7 所示。

### 响应

Response，由服务端返回给客户端，可以分为三部分： 响应状态码（Response Status Code）、响应头（Response Headers）和响应体（Response Body）。

**响应状态码**

表示服务器的响应状态，如 **200 代表服务器正常响应，404 代表页面未找到，500 代表服务器内部发生错误**。

**响应头**

响应头包含了服务器对请求的应答信息，如 Content-Type、Server、Set-Cookie 等。

- Date：标识响应产生的时间。 
- Last-Modified：指定资源的最后修改时间。 
- Content-Encoding：指定响应内容的编码。 
- Server：包含服务器的信息，比如名称、版本号等。 
- Content-Type：文档类型，指定返回的数据类型是什么，如 text/html 代表返回 HTML 文档，application/x-javascript 则代表返回 JavaScript 文件，image/jpeg 则代表返回图片。 
- Set-Cookie：设置 Cookies。响应头中的 Set-Cookie 告诉浏览器需要将此内容放在 Cookies 中，下次请求携带 Cookies 请求。 
- Expires：指定响应的过期时间，可以使代理服务器或浏览器将加载的内容更新到缓存中。如果再次访问时，就可以直接从缓存中加载，降低服务器负载，缩短加载时间。 

**响应体**

**最重要的当属响应体的内容**。响应的正文数据都在响应体中，比如请求网页时，它的响应体就是网页的 HTML 代码；请求一张图片时，它的响应体就是图片的二进制数据。

**爬虫请求网页后，要解析的内容就是响应体**。

> 在浏览器开发者工具中点击 Preview，就可以看到网页的源代码，也就是响应体的内容，它是解析的目标。 
> 
> 在做爬虫时，我们主要通过响应体得到网页的源代码、JSON 数据等，然后从中做相应内容的提取。 

## Web 网页基础
网页可以分为三大部分 —— HTML、CSS 和 JavaScript。

### 节点树及节点间的关系
在 HTML 中，所有标签定义的内容都是节点，它们构成了一个 HTML DOM 树。

> DOM（Document Object Model，文档对象模型）是 W3C（万维网联盟）的标准。它定义了访问 HTML 和 XML 文档的标准。
> 
> W3C 文档对象模型（DOM）是中立于平台和语言的接口，它允许程序和脚本动态地访问和更新文档的内容、结构和样式。

W3C DOM 标准被分为 3 个不同的部分：

- 核心 DOM - 针对任何结构化文档的标准模型 
- XML DOM - 针对 XML 文档的标准模型 
- HTML DOM - 针对 HTML 文档的标准模型

根据 W3C 的 HTML DOM 标准，**HTML 文档中的所有内容都是节点**：

通过 HTML DOM，树中的所有节点均可通过 JavaScript 访问，所有 HTML 节点元素均可被修改，也可以被创建或删除。

节点树中的节点彼此拥有层级关系。常用父（parent）、子（child）和兄弟（sibling）等术语描述这些关系。父节点拥有子节点，同级的子节点被称为兄弟节点。

### 选择器
怎样来定位节点呢？

在 CSS 中可以 **CSS 选择器**来定位节点。常用的3种方式：
- 例如，`<div id="container" ... >`，那么就可以表示为 `#container`，其中 # 开头代表选择 id，其后紧跟 id 的名称。 
- 另外，如果想选择节点`<div class="wrapper" ...>` ，可以使用`.wrapper`，这里以点（.）开头代表选择 class，其后紧跟 class 的名称。 
- 还有一种根据标签名筛选，例如想选择二级标题，直接用 `h2` 即可。

此外，**CSS 选择器还支持嵌套选择**，各个选择器之间加上空格分隔开便可以代表嵌套关系。
> `#container .wrapper p` 则代表先选择 id 为 container 的节点，然后选中其内部的 class 为 wrapper 的节点，然后再进一步选中其内部的 p 节点。

另外，如果不加空格，则代表并列关系。
> `div#container .wrapper p.text` 代表先选择 id 为 container 的 div 节点，然后选中其内部的 class 为 wrapper 的节点，再进一步选中其内部的 class 为 text 的 p 节点。


更多关于 CSS 选择器的语法点[这个链接](https://github.com/Python3WebSpider/Python3WebSpider/blob/master/2.2-Web%E7%BD%91%E9%A1%B5%E5%9F%BA%E7%A1%80.md#224%E9%80%89%E6%8B%A9%E5%99%A8)看表

## 爬虫的基本原理
把互联网比作一张大网，而爬虫（即网络爬虫）便是在网上爬行的蜘蛛。

把网的节点比作一个个网页，爬虫爬到这就相当于访问了该页面，获取了其信息。

可以把节点间的连线比作网页与网页之间的链接关系，这样蜘蛛通过一个节点后，可以顺着节点连线继续爬行到达下一个节点，即通过一个网页继续获取后续的网页。

### 爬虫概述
爬虫就是**获取网页并提取和保存信息的自动化程序**。

1. 获取网页
爬虫首先要做的工作就是获取网页，这里就是<u>获取网页的源代码</u>。源代码里包含了网页的部分有用信息，需要提取想要的信息。

> 向网站的服务器发送一个请求，返回的响应体便是网页源代码。所以，最**关键的部分就是构造一个请求并发送给服务器，然后接收到响应并将其解析出来**。

使用 urllib、requests 等帮助我们实现 HTTP 请求操作，请求和响应都可以用类库提供的数据结构来表示，得到响应之后只需要解析数据结构中的 Body 部分即可。

2. 提取信息
获取网页源代码后，接下来就是**分析网页源代码，从中提取有效的数据**。

首先，最通用的方法便是**采用正则表达式**提取。

另外，由于网页的结构有一定的规则，所以还有一些**根据网页节点属性、CSS 选择器或 XPath 来提取网页信息的库**，如 Beautiful Soup、pyquery、lxml 等。

**提取信息是爬虫非常重要的部分，它可以使杂乱的数据变得条理清晰，以便后续处理和分析数据**。

3. 保存数据
提取信息后，一般会将提取到的数据保存到某处以便后续使用。这里保存形式有多种多样，如可以简单**保存为 TXT 文本或 JSON 文本**，也可以**保存到数据库**，如 MySQL 和 MongoDB 等，也可保存至远程服务器，如借助 SFTP 进行操作等。

### 能抓怎样的数据
常规网页对应着 HTML 代码，比较常见，而最常抓取的便是 HTML 源代码。

另外，可能**有些网页返回的不是 HTML 代码，而是一个 JSON 字符串（其中 API 接口大多采用这样的形式）**，这种格式的数据方便传输和解析，它们同样可以抓取，而且数据提取更加方便。

此外，各种二进制数据，如图片、视频和音频等，也可以利用爬虫，将这些二进制数据抓取下来，然后保存成对应的文件名。

另外，其它各种扩展名的文件，如 CSS、JavaScript 和配置文件等，也可以将其抓取下来。

上述内容其实都**对应各自的 URL，是基于 HTTP 或 HTTPS 协议的**，只要是这种数据，爬虫都可以抓取。

### JavaScript 渲染页面
有时候，在用 urllib 或 requests 抓取网页时，得到的源代码实际和浏览器中看到的不一样。

这是一个非常常见的问题。现在网页越来越多地**采用 Ajax、前端模块化工具来构建，整个网页可能都是由 JavaScript 渲染出来的**，也就是说原始的 HTML 代码就是一个空壳。

因此，使用基本 HTTP 请求库得到的源代码可能跟浏览器中的页面源代码不太一样。对于这样的情况，
- **可以分析其后台 Ajax 接口**
- 也可**使用 Selenium、Splash 这样的库来实现模拟 JavaScript 渲染**。

## 会话和 Cookies
> 在浏览网站的过程中，经常会遇到需要登录的情况，有些页面只有登录之后才可以访问，而且登录之后可以连续访问很多次网站，但是有时候过一段时间就需要重新登录。
> 还有一些网站，在打开浏览器时就自动登录了，而且很长时间都不会失效。
> 这里面涉及**会话（Session）和Cookies** 的相关知识。

区别于静态页面（简单的 HTML 与 CSS 组成），动态网页可以动态解析 URL 中参数的变化，关联数据库并动态呈现不同的页面内容，非常灵活多变。

动态网站还可以实现用户登录和注册的功能，而很多页面是需要登录之后（获得凭证，会话和 Cookies 共同产生的结果）才可以查看的。

### 无状态 HTTP
在了解会话和 Cookies 之前，还需要了解 HTTP 的一个特点，叫作无状态。

HTTP 的无状态是指 **HTTP 协议对事务处理是没有记忆能力的**，也就是说服务器不知道客户端是什么状态。

当客户端向服务器发送请求后，服务器解析此请求，然后返回对应的响应，服务器负责完成这个过程，而且这个过程是完全独立的，服务器不会记录前后状态的变化，也就是缺少状态记录。

这意味着如果后续需要处理前面的信息，则必须重传，这导致需要额外传递一些前面的重复请求，才能获取后续响应。

为了保持前后状态，将前面的请求全部重传一次十分浪费资源，对于这种需要用户登录的页面来说，更是棘手。

这时两个用于保持 HTTP 连接状态的技术就出现了，它们分别是会话和 Cookies。
- 会话在服务端，也就是网站的服务器，用来保存用户的会话信息；
- Cookies 在客户端，也可以理解为浏览器端，有了 Cookies，浏览器在下次访问网页时会自动附带上它发送给服务器，服务器通过识别 Cookies 并鉴定出是哪个用户，然后再判断用户是否是登录状态，然后返回对应的响应。

可以理解为 Cookies 里面保存了登录的凭证，有了它，只需要在下次请求携带 Cookies 发送请求而不必重新输入用户名、密码等信息重新登录了。

因此在爬虫中，有时候处理需要登录才能访问的页面时，一般会直接**将登录成功后获取的 Cookies 放在请求头里面直接请求，而不必重新模拟登录**。

1. 会话
会话，其本来的含义是指有始有终的一系列动作 / 消息。比如，打电话时，从拿起电话拨号到挂断电话这中间的一系列过程可以称为一个会话。

而在 Web 中，**会话对象用来存储特定用户会话所需的属性及配置信息**。这样，当用户在应用程序的 Web 页之间跳转时，存储在会话对象中的变量将不会丢失，而是在整个用户会话中一直存在下去。

当用户请求来自应用程序的 Web 页时，如果该用户还没有会话，则 Web 服务器将自动创建一个会话对象。当会话过期或被放弃后，服务器将终止该会话。

2. Cookies
Cookies 指某**些网站为了辨别用户身份、进行会话跟踪而存储在用户本地终端上的数据**。

**会话维持**

怎样利用 Cookies 保持状态呢？

当客户端第一次请求服务器时，服务器会返回一个响应头中带有 **Set-Cookie** 字段的响应给客户端，用来标记是哪一个用户，客户端浏览器会把 Cookies 保存起来。当浏览器下一次再请求该网站时，浏览器会把此 Cookies 放到请求头一起提交给服务器，Cookies 携带了会话 ID 信息，服务器检查该 Cookies 即可找到对应的会话是什么，然后再判断会话来以此来辨认用户状态。

在成功登录某个网站时，服务器会告诉客户端设置哪些 Cookies 信息，在后续访问页面时客户端会把 Cookies 发送给服务器，服务器再找到对应的会话加以判断。如果会话中的某些设置登录状态的变量是有效的，那就证明用户处于登录状态，此时返回登录之后才可以查看的网页内容，浏览器再进行解析便可以看到了。

反之，如果传给服务器的 Cookies 是无效的，或者会话已经过期了，我们将不能继续访问页面，此时可能会收到错误的响应或者跳转到登录页面重新登录。

所以，**Cookies 和会话需要配合，一个处于客户端，一个处于服务端，二者共同协作，就实现了登录会话控制**。

**属性结构**

以知乎为例，在浏览器开发者工具中打开 **Application** 选项卡，然后在左侧会有一个 **Storage** 部分，最后一项即为 **Cookies**，将其点开。

- Name，即该 Cookie 的名称。Cookie 一旦创建，名称便不可更改 
- Value，即该 Cookie 的值。如果值为 Unicode 字符，需要为字符编码。如果值为二进制数据，则需要使用 BASE64 编码。 
- Max Age，即该 Cookie 失效的时间，单位秒，也常和 Expires 一起使用，通过它可以计算出其有效时间。Max Age 如果为正数，则该 Cookie 在 Max Age 秒之后失效。如果为负数，则关闭浏览器时 Cookie 即失效，浏览器也不会以任何形式保存该 Cookie。 
- Path，即该 Cookie 的使用路径。如果设置为 /path/，则只有路径为 /path/ 的页面可以访问该 Cookie。如果设置为 /，则本域名下的所有页面都可以访问该 Cookie。 
- Domain，即可以访问该 Cookie 的域名。例如如果设置为 .zhihu.com，则所有以 zhihu.com，结尾的域名都可以访问该 Cookie。 
- Size 字段，即此 Cookie 的大小。 
- Http 字段，即 Cookie 的 httponly 属性。若此属性为 true，则只有在 HTTP Headers 中会带有此 Cookie 的信息，而不能通过 document.cookie 来访问此 Cookie。 
- Secure，即该 Cookie 是否仅被使用安全协议传输。安全协议。安全协议有 HTTPS，SSL 等，在网络上传输数据之前先将数据加密。默认为 false。

**会话 Cookie 和持久 Cookie**

会话 Cookie 就是把 Cookie 放在浏览器内存里，浏览器在关闭之后该 Cookie 即失效；

持久 Cookie 则会保存到客户端的硬盘中，下次还可以继续使用，用于长久保持用户登录状态。

> 其实严格来说，没有会话 Cookie 和持久 Cookie 之分，只是由 Cookie 的 Max Age 或 Expires 字段决定了过期的时间。

**常见误区**

在谈论会话机制的时候，常常听到这样一种误解 ——“关闭浏览器，会话就消失了”。

当我们关闭浏览器时，浏览器不会主动在关闭之前通知服务器它将要关闭，所以服务器根本不会有机会知道浏览器已经关闭。

之所以会有这种错觉，是因为大部分会话机制都使用会话 Cookie 来保存会话 ID 信息，而关闭浏览器后 Cookies 就消失了，再次连接服务器时，也就无法找到原来的会话了。如果服务器设置的 Cookies 保存到硬盘上，或者使用某种手段改写浏览器发出的 HTTP 请求头，把原来的 Cookies 发送给服务器，则再次打开浏览器，仍然能够找到原来的会话 ID，依旧还是可以保持登录状态的。

而且恰恰是由于关闭浏览器不会导致会话被删除，这就需要服务器为会话设置一个失效时间，当距离客户端上一次使用会话的时间超过这个失效时间时，服务器就可以认为客户端已经停止了活动，才会把会话删除以节省存储空间。


### 代理的基本原理
在做爬虫的过程中经常会遇到这样的情况，最初爬虫正常运行，正常抓取数据，一切看起来都是那么美好，然而一杯茶的功夫可能就会出现错误，比如 403 Forbidden，这时候打开网页一看，可能会看到 “您的 IP 访问频率太高” 这样的提示。

出现这种现象的原因是网站采取了一些反爬虫措施。比如，服务器会检测某个 IP 在单位时间内的请求次数，如果超过了这个阈值，就会直接拒绝服务，返回一些错误信息，这种情况可以称为封 IP。

> 既然服务器检测的是某个 IP 单位时间的请求次数，那么借助某种方式来伪装我们的 IP，让服务器识别不出是由我们本机发起的请求，不就可以成功防止封 IP 了吗？
> 
> 一种有效的方式就是使用代理，后面会详细说明代理的用法。在这之前，需要先了解下代理的基本原理，它是怎样实现 IP 伪装的呢？

**基本原理**
代理实际上指的就是代理服务器，英文叫作 proxy server，它的功能是**代理网络用户去取得网络信息**。形象地说，它是网络信息的中转站。

在我们正常请求一个网站时，是发送了请求给 Web 服务器，Web 服务器把响应传回给我们。如果设置了代理服务器，实际上就是在本机和服务器之间搭建了一个桥，此时本机不是直接向 Web 服务器发起请求，而是向代理服务器发出请求，请求会发送给代理服务器，然后由代理服务器再发送给 Web 服务器，接着由代理服务器再把 Web 服务器返回的响应转发给本机。

这样我们同样可以正常访问网页，但这个过程中 Web 服务器识别出的真实 IP 就不再是我们本机的 IP 了，就成功实现了 IP 伪装，这就是代理的基本原理。

**代理的作用**

1. 突破自身 IP 访问限制，访问一些平时不能访问的站点。 
2. 访问一些单位或团体内部资源，如使用教育网内地址段免费代理服务器，就可以用于对教育网开放的各类 FTP 下载上传，以及各类资料查询共享等服务。 
3. 提高访问速度，通常代理服务器都设置一个较大的硬盘缓冲区，当有外界的信息通过时，同时也将其保存到缓冲区中，当其他用户再访问相同的信息时， 则直接由缓冲区中取出信息，传给用户，以提高访问速度。 
4. 隐藏真实 IP，上网者也可以通过这种方法隐藏自己的 IP，免受攻击，对于爬虫来说，我们用代理就是为了隐藏自身 IP，防止自身的 IP 被封锁。

**爬虫代理**

对于爬虫来说，由于爬虫爬取速度过快，在爬取过程中可能遇到同一个 IP 访问过于频繁的问题，此时网站就会让我们输入验证码登录或者直接封锁 IP，这样会给爬取带来极大的不便。

使用代理隐藏真实的 IP，让服务器误以为是代理服务器在请求自己。这样在爬取过程中通过不断更换代理，就不会被封锁，可以达到很好的爬取效果。

**代理分类**


1. 根据协议区分
   - FTP 代理服务器，主要用于访问 FTP 服务器，一般有上传、下载以及缓存功能，端口一般为 21、2121 等。 
   - HTTP 代理服务器，主要用于访问网页，一般有内容过滤和缓存功能，端口一般为 80、8080、3128 等。 
   - SSL/TLS 代理，主要用于访问加密网站，一般有 SSL 或 TLS 加密功能（最高支持 128 位加密强度），端口一般为 443。 
   - RTSP 代理，主要用于 Realplayer 访问 Real 流媒体服务器，一般有缓存功能，端口一般为 554。 
   - Telnet 代理，主要用于 telnet 远程控制（黑客入侵计算机时常用于隐藏身份），端口一般为 23。 
   - POP3/SMTP 代理，主要用于 POP3/SMTP 方式收发邮件，一般有缓存功能，端口一般为 110/25。 
   - SOCKS 代理，只是单纯传递数据包，不关心具体协议和用法，所以速度快很多，一般有缓存功能，端口一般为 1080。SOCKS 代理协议又分为 SOCKS4 和 SOCKS5，SOCKS4 协议只支持 TCP，而 SOCKS5 协议支持 TCP 和 UDP，还支持各种身份验证机制、服务器端域名解析等。简单来说，SOCK4 能做到的 SOCKS5 都可以做到，但 SOCKS5 能做到的 SOCK4 不一定能做到。
2. 根据匿名程度区分
   - 高度匿名代理，高度匿名代理会将数据包原封不动的转发，在服务端看来就好像真的是一个普通客户端在访问，而记录的 IP 是代理服务器的 IP。 
   - 普通匿名代理，普通匿名代理会在数据包上做一些改动，服务端上有可能发现这是个代理服务器，也有一定几率追查到客户端的真实 IP。代理服务器通常会加入的 HTTP 头有 HTTP_VIA 和 HTTP_X_FORWARDED_FOR。 
   - 透明代理，透明代理不但改动了数据包，还会告诉服务器客户端的真实 IP。这种代理除了能用缓存技术提高浏览速度，能用内容过滤提高安全性之外，并无其他显著作用，最常见的例子是内网中的硬件防火墙。 
   - 间谍代理，间谍代理指组织或个人创建的，用于记录用户传输的数据，然后进行研究、监控等目的代理服务器。

**常见代理设置**
- 使用网上的免费代理，最好使用高匿代理，使用前抓取下来筛选一下可用代理，也可以进一步维护一个代理池。 
- 使用付费代理服务，互联网上存在许多代理商，可以付费使用，质量比免费代理好很多。 
- ADSL 拨号，拨一次号换一次 IP，稳定性高，也是一种比较有效的解决方案。
